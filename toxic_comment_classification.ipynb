{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Start Project: look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# vocab = torchtext.vocab.build_vocab_from_iterator(train_X[\"comment_text\"],min_freq=5,specials=[\"<unk>\",\"<pd>\",\"<eos>\"],special_first=True)\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Class ##\n",
    "We will use this Dataset for training,and testing. we made it very general but usually we will do more preprossecing in the __init__ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self,file,tokenizer, transform = None,target_transform = None):\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        data = pd.read_csv(file)\n",
    "        X = data[\"comment_text\"].apply(lambda row: tokenizer(row) + [\"<eos>\"]).values\n",
    "        d = data.values\n",
    "        y = None\n",
    "        if d.shape[1] > 2:\n",
    "            y = data.values[:,2:].astype(np.int32)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = None\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        if self.vocab is None:\n",
    "            self.vocab = build_vocab_from_iterator(self.X,min_freq=5,specials=[\"<unk>\",\"<pd>\",\"<eos>\"],special_first=True)\n",
    "            self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        return self.vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx]\n",
    "        label = self.y[idx] if self.y is not None else None\n",
    "        # if self.transform:\n",
    "        #     item = self.transform(item)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return item,label\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# def convert_data(file,return_vocab = True):\n",
    "#     data = pd.read_csv(file)\n",
    "#     tokenizer = get_tokenizer(\"basic_english\")\n",
    "#     X = data[\"comment_text\"].apply(lambda row: tokenizer(row) + [\"<eos>\"]).values\n",
    "#     d = data.values\n",
    "#     y = None\n",
    "#     if d.shape[1] > 2:\n",
    "#         y = data.values[:,2:]\n",
    "#     if return_vocab:\n",
    "#         vocab = torchtext.vocab.build_vocab_from_iterator(X,min_freq=5,specials=[\"<unk>\",\"<pd>\",\"<eos>\"],special_first=True)\n",
    "#         vocab.set_default_index(vocab[\"<unk>\"])\n",
    "#         return X,y,vocab\n",
    "#     return X,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y,vocab = convert_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.transforms import VocabTransform,ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train = TransformDataset(\"train.csv\",tokenizer)\n",
    "vocab = train.get_vocab()\n",
    "transform = Compose([VocabTransform(vocab),ToTensor(vocab([\"<pd>\"])[0],dtype=torch.int32)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    print(batch[0][1])\n",
    "    X = [item[0] for item in batch]\n",
    "    y = np.array([item[1] for item in batch])\n",
    "    X = transform(X)\n",
    "    return X.to(device),torch.tensor(y,dtype=torch.int32,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n",
      "tensor([[   61,  4722,  3863,    83,   114,    12,  2982,    63,    19,   453,\n",
      "            90,    16,    12,  1037,    34,    28,    23,    40, 13438,    21,\n",
      "             4,   645,   292,    14,     4,  2256,     8,     4,   723, 16037,\n",
      "            14,     8,  5867,  2716,     5,   120,    25,     0,     5,     0,\n",
      "             5,    36,     0,     0, 14982,  4602,     5,     9,    14,   102,\n",
      "           723,  4645,     0,  2256,     5,     0,     0,     5,     9,     0,\n",
      "             0,     3,  9844,     0,    54,  2635,    20,     6,   951,   851,\n",
      "             8,    83,     4,   723,   755,    36,   790,     8,   119,  2256,\n",
      "            49,  1180,    41, 11132,   384,    54,    68,   724,     6,  1133,\n",
      "            16,    12,  1037,    34,    66,    12,   384,    14,   404,   541,\n",
      "          1294,    18,     4,   645,     3,    14,    19,    54,    68,  3321,\n",
      "             6,  1037,    34,   663,   772,     3,   100,   301,   645,    55,\n",
      "           106,   102,  3998,    84,     6,   343,    12,     0,     5,   673,\n",
      "          1427,    34,    18,   102,  2458,     9, 23175,    17,  2667,     8,\n",
      "          4650,   990,     8,   243,  1294,     3,  2219,    22,    11,     5,\n",
      "            38,    84,     3,     2],\n",
      "        [ 4331,   139,   749,    10,   321, 43945,    66,  2518,    11,    26,\n",
      "            72,    96,  5603,    66,     7,   110,    12,  1247,   162,   894,\n",
      "            23,   118, 11744,     0,  9227,     0,    21,    66,    13,    52,\n",
      "           225,    16, 42051,    48,    10,    10,     0,     5,     0,     9,\n",
      "             4,    73, 11840,    65,    88,    26,    61,   529,    22,    66,\n",
      "          3565,     0,   294,   335,    66,    64,   115,   133,    31,    12,\n",
      "          1856,    28,    85,   609,    15,    90,    18,  2038,     9,    19,\n",
      "            57,    24,    12,   289,   130,    23,   258,     6,   108,     4,\n",
      "             0,   179,   162,    12,   255,   457,    10,     0,    10,    66,\n",
      "          2771,    83,   640,     4,  4331,    39,    54,    68,  1738,  1063,\n",
      "             4,   568,    91,    11,    93,   599,     3,     3,     3,     3,\n",
      "           643,     5,    23,    51,   999,   517,    21,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [   75,  1957,   133,    66,    21,   196,    24,   109,     6,    76,\n",
      "            33,   123,    45,    24,   190,     6,  6416,   390,    11,    10,\n",
      "           113,  2382,  2871,    23,    51,   999,   517,    21,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [  452,  2166,    93,  1048,   361,    94,    28,     0,     4, 13360,\n",
      "          1210,    22,  8321,    13,    63,    12,  6113,    39,    22,  2166,\n",
      "           136,  3381,    10,    29,  6267,     5,     9,    14,  6113,    39,\n",
      "            54,    12,   180,   364,   407,    19,   305,    13,   109,   172,\n",
      "             3,  1284,   555,   145,    50,     4,     0,   180,    13,     3,\n",
      "             9,    11,    10,   116,   364,    14,    65,    69,   160,    14,\n",
      "            25,    12,   532,   475,     5,    62,    80,    11,    79,    15,\n",
      "            10,    29,   497,   315,     6,   160,    22,    38,    28,    23,\n",
      "            36,  6657,   241,    15,    22,    38,    21,    57,    11,   160,\n",
      "            14,   180,    22,    14,     0,    22,    38,    25,    12,   532,\n",
      "           475,    28,     7,   282,    15,    10,    29,    62,    12,  1302,\n",
      "             8,     4,   305,     3,    86,   114,    15,   135,    12,   439,\n",
      "          1603,     8,   285,     9,    20,    62,    12,   180,   364,     0,\n",
      "             8,     0,    20,    14,    14,    57,    24,   225,  2611,     6,\n",
      "           160,    37,    62,    25,    41,   316,     3,     2,     1,     1,\n",
      "             1,     1,     1,     1]], device='cuda:0', dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(train,batch_size=4,shuffle=True,collate_fn=collate_batch)\n",
    "\n",
    "for X,y in dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the training data. now we would like to get some embedding for the words. I will try to use self attention to create context rich embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
